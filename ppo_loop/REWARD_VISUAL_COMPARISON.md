# Visual Comparison: Three Reward Schemes

## The Key Insight: What Are We Comparing?

```
Time ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>

Agent Trajectory (with perturbations):
Step 0: cost=10   (added request 0 with action 5)
Step 1: cost=24   (added request 1 with action 8)
Step 2: cost=40   (added request 2 with action 3)
Step 3: cost=58   (added request 3 with action 12)
...

Baseline Trajectory (always action 12 = no perturbation):
Step 0: cost=12   (added request 0 with action 12)
Step 1: cost=28   (added request 1 with action 12)
Step 2: cost=48   (added request 2 with action 12)
Step 3: cost=65   (added request 3 with action 12)
...
```

**Important**: Both trajectories process the SAME sequence of requests, but with DIFFERENT actions!

---

## Option 1: Current Scheme (Temporal Difference)

### What it compares
```
Agent Trajectory Only:
    cost=10 ‚îÄ‚îÄ> cost=24 ‚îÄ‚îÄ> cost=40 ‚îÄ‚îÄ> cost=58
       ‚îÇ            ‚îÇ            ‚îÇ            ‚îÇ
       ‚îÇ -10        ‚îÇ -14        ‚îÇ -16        ‚îÇ -18
       ‚ñº            ‚ñº            ‚ñº            ‚ñº
    reward       reward       reward       reward
```

**Formula**: `reward_t = cost_{t-1} - cost_t`

### Example
| Step | Cost | Reward | Interpretation |
|------|------|--------|----------------|
| 0 | 10 | **-10** | Adding request 0 increased cost by 10 km |
| 1 | 24 | **-14** | Adding request 1 increased cost by 14 km |
| 2 | 40 | **-16** | Adding request 2 increased cost by 16 km |

‚ùå **Problem**: Always negative! Agent never gets positive reinforcement.
‚ùå **Problem**: Doesn't compare to baseline.

---

## Option 2: Percentage Improvement (Cumulative Comparison)

### What it compares
```
Agent:    cost=10      cost=24      cost=40      cost=58
          ‚îÇ            ‚îÇ            ‚îÇ            ‚îÇ
          ‚îú‚îÄ compare ‚îÄ‚îÄ‚î§            ‚îÇ            ‚îÇ
          ‚îÇ            ‚îú‚îÄ compare ‚îÄ‚îÄ‚î§            ‚îÇ
Baseline: cost=12      cost=28      cost=48      cost=65
          ‚îÇ            ‚îÇ            ‚îÇ            ‚îÇ
          ‚ñº            ‚ñº            ‚ñº            ‚ñº
      +16.7%       +14.3%       +16.7%       +10.8%
```

**Formula**: `reward_t = ((baseline_cost_t - agent_cost_t) / baseline_cost_t) √ó 100`

### Example
| Step | Agent Cost | Baseline Cost | Œî | Reward (%) | Interpretation |
|------|------------|---------------|---|------------|----------------|
| 0 | 10 | 12 | +2 | **+16.7%** | Agent is 16.7% better overall |
| 1 | 24 | 28 | +4 | **+14.3%** | Agent is 14.3% better overall |
| 2 | 40 | 48 | +8 | **+16.7%** | Agent is 16.7% better overall |

‚ö†Ô∏è **Issue**: Cumulative! Reward at step 2 includes benefits from steps 0 and 1.
‚ö†Ô∏è **Issue**: Credit assignment problem - which past action caused the improvement?
‚úÖ **Good**: Normalized percentage, positive rewards possible.

---

## Option 3: Step-wise Marginal Cost Difference (Your Suggestion!)

### What it compares
```
Agent Trajectory:
    Œî+10         Œî+14         Œî+16         Œî+18
    ‚îÄ‚îÄ‚îÄ‚îÄ>        ‚îÄ‚îÄ‚îÄ‚îÄ>        ‚îÄ‚îÄ‚îÄ‚îÄ>        ‚îÄ‚îÄ‚îÄ‚îÄ>
cost=10      cost=24      cost=40      cost=58

Baseline Trajectory:
    Œî+12         Œî+16         Œî+20         Œî+17
    ‚îÄ‚îÄ‚îÄ‚îÄ>        ‚îÄ‚îÄ‚îÄ‚îÄ>        ‚îÄ‚îÄ‚îÄ‚îÄ>        ‚îÄ‚îÄ‚îÄ‚îÄ>
cost=12      cost=28      cost=48      cost=65

Rewards (baseline Œî - agent Œî):
    +2           +2           +4           -1
```

**Formula**: `reward_t = (baseline_cost_t - baseline_cost_{t-1}) - (agent_cost_t - agent_cost_{t-1})`

### Example
| Step | Agent Œî | Baseline Œî | Reward | Interpretation |
|------|---------|------------|--------|----------------|
| 0 | +10 | +12 | **+2** | My action made request 0 cheaper by 2 km! |
| 1 | +14 | +16 | **+2** | My action made request 1 cheaper by 2 km! |
| 2 | +16 | +20 | **+4** | My action made request 2 cheaper by 4 km! |
| 3 | +18 | +17 | **-1** | My action made request 3 more expensive by 1 km |

‚úÖ **Perfect**: Each reward directly measures THIS action's effect!
‚úÖ **Perfect**: Can be positive (good action) or negative (bad action)!
‚úÖ **Perfect**: Sum of rewards = final improvement!

---

## The Mathematical Beauty of Option 3

### Proof: Step-wise rewards sum to episode goal

Given:
- Agent costs: [c‚ÇÄ, c‚ÇÅ, c‚ÇÇ, ..., c‚ÇÇ‚Çâ]
- Baseline costs: [b‚ÇÄ, b‚ÇÅ, b‚ÇÇ, ..., b‚ÇÇ‚Çâ]
- Both start at 0: c‚Çã‚ÇÅ = b‚Çã‚ÇÅ = 0

**Option 3 total reward:**
```
Œ£ reward_t = Œ£ [(b‚Çú - b‚Çú‚Çã‚ÇÅ) - (c‚Çú - c‚Çú‚Çã‚ÇÅ)]

           = Œ£ (b‚Çú - b‚Çú‚Çã‚ÇÅ) - Œ£ (c‚Çú - c‚Çú‚Çã‚ÇÅ)

           = (b‚ÇÄ - b‚Çã‚ÇÅ) + (b‚ÇÅ - b‚ÇÄ) + ... + (b‚ÇÇ‚Çâ - b‚ÇÇ‚Çà)
             - (c‚ÇÄ - c‚Çã‚ÇÅ) - (c‚ÇÅ - c‚ÇÄ) - ... - (c‚ÇÇ‚Çâ - c‚ÇÇ‚Çà)

           = b‚ÇÇ‚Çâ - b‚Çã‚ÇÅ - c‚ÇÇ‚Çâ + c‚Çã‚ÇÅ  (telescoping sum!)

           = b‚ÇÇ‚Çâ - c‚ÇÇ‚Çâ  (since b‚Çã‚ÇÅ = c‚Çã‚ÇÅ = 0)

           = Final baseline cost - Final agent cost
```

**üéâ The sum of step-wise marginal rewards = total episode improvement!**

This means:
- ‚úÖ Maximizing step-wise rewards ‚Üí maximizing episode performance
- ‚úÖ Perfect credit assignment: Each step gets exactly the credit it deserves
- ‚úÖ No "leak" or "double counting" of rewards

---

## Side-by-side Example Episode

### Setup
- 5 requests
- Agent chooses actions: [3, 8, 5, 12, 7]
- Baseline always chooses: [12, 12, 12, 12, 12]

### Option 1: Current (Temporal)
```
Step | Agent Cost | Reward | Cumulative
-----|------------|--------|------------
  0  |     10     |  -10   |    -10
  1  |     24     |  -14   |    -24
  2  |     40     |  -16   |    -40
  3  |     58     |  -18   |    -58
  4  |     72     |  -14   |    -72
```
**Episode reward**: -72 ‚ùå (always negative)

### Option 2: Percentage (Cumulative)
```
Step | Agent | Baseline | % Improve | Cumulative
-----|-------|----------|-----------|------------
  0  |   10  |    12    |  +16.7%   |   +16.7%
  1  |   24  |    28    |  +14.3%   |   +31.0%
  2  |   40  |    48    |  +16.7%   |   +47.7%
  3  |   58  |    65    |  +10.8%   |   +58.5%
  4  |   72  |    78    |   +7.7%   |   +66.2%
```
**Episode reward**: +66.2% ‚ö†Ô∏è (positive, but cumulative/non-Markovian)

### Option 3: Marginal Cost Difference
```
Step | Agent Œî | Baseline Œî | Reward | Cumulative
-----|---------|------------|--------|------------
  0  |   +10   |     +12    |   +2   |     +2
  1  |   +14   |     +16    |   +2   |     +4
  2  |   +16   |     +20    |   +4   |     +8
  3  |   +18   |     +17    |   -1   |     +7
  4  |   +14   |     +13    |   -1   |     +6
```
**Episode reward**: +6 km ‚úÖ (positive when good, equals final improvement!)

**Verify**: Final costs are agent=72, baseline=78, difference = 78-72 = 6 ‚úÖ

---

## Summary Table

| Criterion | Option 1<br>(Current) | Option 2<br>(Percentage) | Option 3<br>(Marginal) |
|-----------|-------------------|---------------------|-------------------|
| **Reward Sign** | Always negative ‚ùå | Can be positive ‚úÖ | Can be positive ‚úÖ |
| **Credit Assignment** | Poor ‚ùå | Poor (cumulative) ‚ö†Ô∏è | Perfect ‚úÖ |
| **Normalized** | No ‚ùå | Yes ‚úÖ | No ‚ö†Ô∏è |
| **Markovian** | Yes ‚úÖ | No (cumulative) ‚ùå | Yes ‚úÖ |
| **Compares to Baseline** | No ‚ùå | Yes ‚úÖ | Yes ‚úÖ |
| **Episode Reward = Goal** | No ‚ùå | No ‚ùå | Yes ‚úÖ |
| **Implementation** | Current | Easy (10 lines) | Easy (15 lines) |

---

## Recommendation: **Option 3** üèÜ

**Why?**
1. ‚úÖ **Best credit assignment**: Each step's reward = that action's marginal benefit
2. ‚úÖ **Mathematically elegant**: Sum of rewards = episode goal
3. ‚úÖ **Positive reinforcement**: Good actions get positive rewards
4. ‚úÖ **True counterfactual**: "What if I chose action 12 instead?"
5. ‚úÖ **Markovian**: Reward depends on current state-action, not history
6. ‚úÖ **Easy to implement**: ~15 lines of code

**When it's best:**
- Agent beats baseline ‚Üí Positive rewards encourage policy
- Agent worse than baseline ‚Üí Negative rewards discourage policy
- Each step gets credit for its own contribution

**Expected outcome:**
- Faster learning
- Better exploration (knows when perturbation helps vs hurts)
- More interpretable (can see which actions are beneficial)

Would you like me to implement this?
