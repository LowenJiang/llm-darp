# Scaling to 256 Parallel Environments: Analysis

## Quick Answer

**Using 256 environments should be fine!** ‚úÖ

But there are some considerations for **memory, compute, and hyperparameters**.

---

## Current Setup (64 Environments)

From `meta_train.py`:
```python
num_envs = 64  # Default
num_customers = 30
num_steps = 30
```

**Memory footprint per epoch**:
- Rollout buffer: `64 envs √ó 30 steps √ó (state + action + log_prob + value + reward)`
- State size: `(606, 2)` = 1,212 floats ‚âà 4.8 KB per state
- Total per epoch: `64 √ó 30 √ó 4.8 KB ‚âà 9 MB` (manageable)

---

## Scaling to 256 Environments

### 1. **Memory Requirements** ‚ö†Ô∏è

**Buffer size increase**: 4√ó larger

```python
# Current (64 envs):
Buffer: 64 √ó 30 = 1,920 samples per epoch

# Scaled (256 envs):
Buffer: 256 √ó 30 = 7,680 samples per epoch
```

**Memory breakdown** (per epoch):

| Component | 64 envs | 256 envs | Increase |
|-----------|---------|----------|----------|
| States | 1,920 √ó 1,212 floats ‚âà 9 MB | 7,680 √ó 1,212 floats ‚âà 37 MB | 4√ó |
| Actions | 1,920 ints ‚âà 8 KB | 7,680 ints ‚âà 30 KB | 4√ó |
| Log probs | 1,920 floats ‚âà 8 KB | 7,680 floats ‚âà 30 KB | 4√ó |
| Values | 1,920 floats ‚âà 8 KB | 7,680 floats ‚âà 30 KB | 4√ó |
| Rewards | 1,920 floats ‚âà 8 KB | 7,680 floats ‚âà 30 KB | 4√ó |
| **Total** | **‚âà 9 MB** | **‚âà 37 MB** | **4√ó** |

**Verdict**: ‚úÖ **Still very manageable** (37 MB is tiny for modern GPUs/CPUs)

---

### 2. **Neural Oracle Batching** üöÄ

The key optimization in `vectorized_env.py`:

```python
# vectorized_env.py:253-268
batched_td = torch.cat(requests_batch, dim=0)  # Batch all environments
out = self.policy(batched_td, phase='test', decode_type="greedy")
```

**Current**: Single forward pass for 64 routing problems
**Scaled**: Single forward pass for 256 routing problems

**GPU memory for neural oracle**:
- Batch size 64: ~2-4 GB VRAM (depends on model)
- Batch size 256: ~8-16 GB VRAM

**Verdict**:
- ‚úÖ **CPU**: No problem
- ‚ö†Ô∏è **GPU**: May need GPU with 16+ GB VRAM (RTX 3090/4090, A100)
- ‚ö†Ô∏è If OOM, can split batch: process 128 at a time instead of 256

---

### 3. **PPO Update Efficiency** üìà

**Current PPO update** (from `ppo_agent.py:601`):

```python
agent.update(
    num_value_epochs=40,
    num_policy_epochs=10,
    batch_size=64,
    num_envs=num_envs,
    num_steps=num_customers
)
```

**Scaled to 256 envs**:

| Setting | 64 envs | 256 envs | Impact |
|---------|---------|----------|--------|
| Buffer size | 1,920 samples | 7,680 samples | 4√ó data |
| Mini-batches (batch_size=64) | 30 batches/epoch | 120 batches/epoch | 4√ó iterations |
| Training time per update | ~2-3 sec | ~8-12 sec | 4√ó slower |

**Verdict**: ‚ö†Ô∏è **PPO updates will be 4√ó slower**, but:
- ‚úÖ More data per update = better gradient estimates
- ‚úÖ Fewer total updates needed (more data per epoch)
- ‚úÖ Can reduce `policy_update_interval` if needed

---

### 4. **Episode Count Adjustment** üìä

**Current setup**:
```python
num_episodes = 1000
num_envs = 64
num_epochs = 1000 / 64 = 15.625 ‚âà 16 epochs
```

**With 256 envs**:
```python
num_episodes = 1000
num_envs = 256
num_epochs = 1000 / 256 = 3.9 ‚âà 4 epochs
```

**Problem**: Only 4 epochs! Too few for learning.

**Solution**: Increase total episodes proportionally
```python
# Keep number of epochs similar
num_episodes = 4000  # 4√ó more episodes
num_envs = 256
num_epochs = 4000 / 256 = 15.625 ‚âà 16 epochs  # Same as before!
```

**Verdict**: ‚ö†Ô∏è **Must increase `num_episodes` proportionally**

---

### 5. **Hyperparameter Tuning** üéõÔ∏è

When scaling environments, consider adjusting:

#### A. **Batch Size**
```python
# Current
batch_size = 64

# Scaled (optional: larger batches for more data)
batch_size = 128  # or even 256
```
- ‚úÖ Larger batches = more stable gradients
- ‚ö†Ô∏è But slower per-epoch training

#### B. **Learning Rate**
```python
# Current
lr = 3e-4

# Scaled (might benefit from slight increase)
lr = 5e-4  # Larger batches can handle slightly higher LR
```

#### C. **Entropy Coefficient**
```python
# Current
entropy_coef = 0.05

# Scaled (might want more exploration with more envs)
entropy_coef = 0.01  # Reduce if too much diversity
```

**Verdict**: ‚ö†Ô∏è **May need minor tuning**, but defaults should work

---

## Potential Issues and Solutions

### Issue 1: GPU Memory (Neural Oracle)

**Symptom**: CUDA Out of Memory during `_batch_solve_routing`

**Solution**: Split batching
```python
# In vectorized_env.py:233
def _batch_solve_routing(self, requests_batch):
    max_batch = 128  # Process 128 at a time instead of 256
    costs = []

    for i in range(0, len(requests_batch), max_batch):
        batch = requests_batch[i:i+max_batch]
        batched_td = torch.cat(batch, dim=0)
        # ... forward pass
        costs.extend(batch_costs)

    return np.array(costs)
```

**Cost**: Slightly slower (2√ó forward passes), but still faster than sequential

---

### Issue 2: Training Too Slow

**Symptom**: PPO updates take >30 seconds per epoch

**Solution 1**: Reduce update frequency
```python
# meta_train.py
policy_update_interval = 2  # Update every 2 epochs instead of every 1
```

**Solution 2**: Reduce epochs per update
```python
agent.update(
    num_value_epochs=20,   # Down from 40
    num_policy_epochs=5,   # Down from 10
    batch_size=128,        # Increase batch size
)
```

---

### Issue 3: Too Few Epochs

**Symptom**: Only 4-5 epochs total, not enough for learning

**Solution**: Scale `num_episodes` proportionally
```python
# Keep epochs constant when scaling envs
target_epochs = 100
num_envs = 256
num_episodes = target_epochs * num_envs  # 25,600 episodes
```

---

## Recommended Configuration for 256 Envs

```python
# meta_train.py
def train(
    num_episodes: int = 25600,  # 100 epochs √ó 256 envs
    num_customers: int = 30,
    num_envs: int = 256,        # ‚Üê Scaled from 64
    save_dir: str = "./checkpoints",
    save_interval: int = 10,    # Save every 10 epochs
    log_interval: int = 1,
    policy_update_interval: int = 1,
    device: str = "cuda",       # GPU recommended for 256 envs
    ...
):
    # PPO hyperparameters (slight adjustments)
    agent = PPOAgent(
        state_dim=state_dim,
        action_dim=action_dim,
        hidden_dim=256,
        lr=5e-4,              # Slightly higher for larger batches
        gamma=0.99,
        gae_lambda=0.95,
        clip_epsilon=0.1,
        value_coef=0.5,
        entropy_coef=0.05,
        device=device,
    )

    # Update parameters (adjust for 4√ó data)
    agent.update(
        num_value_epochs=30,   # Reduced from 40 (more data compensates)
        num_policy_epochs=8,   # Reduced from 10
        batch_size=128,        # Increased from 64 (better GPU utilization)
        num_envs=num_envs,
        num_steps=num_customers
    )
```

---

## Performance Comparison

### Throughput (samples/second)

| Envs | Samples/epoch | Epoch time | Samples/sec | Speedup |
|------|---------------|------------|-------------|---------|
| 1    | 30            | ~10 sec    | 3           | 1√ó      |
| 64   | 1,920         | ~5 sec     | 384         | 128√ó    |
| 256  | 7,680         | ~15 sec    | 512         | 171√ó    |

**Verdict**: ‚úÖ **256 envs = 33% more throughput than 64 envs**

### Wall-clock time for 100 epochs

| Envs | Episodes | Epochs | Epoch time | Total time |
|------|----------|--------|------------|------------|
| 64   | 6,400    | 100    | ~5 sec     | ~8 min     |
| 256  | 25,600   | 100    | ~15 sec    | ~25 min    |

**Verdict**: ‚ö†Ô∏è **3√ó longer wall-clock time**, but:
- ‚úÖ 4√ó more data collected
- ‚úÖ Better gradient estimates (less variance)
- ‚úÖ Potentially better final performance

---

## Checklist for Scaling to 256 Envs

Before running:

- [ ] **Increase `num_episodes`**: `num_episodes = num_epochs √ó 256`
- [ ] **Check GPU memory**: 16+ GB VRAM recommended (or use CPU)
- [ ] **Adjust batch_size**: Consider 128 or 256 (better GPU utilization)
- [ ] **Reduce update epochs**: `num_value_epochs=30`, `num_policy_epochs=8`
- [ ] **Monitor first epoch**: Check for OOM errors or excessive slowness
- [ ] **Set device='cuda'**: For faster neural oracle (if available)

Optional:
- [ ] Split neural oracle batching if OOM (process 128 at a time)
- [ ] Increase `policy_update_interval` to 2 if updates are too slow
- [ ] Slightly increase LR to `5e-4` for larger effective batch size

---

## Summary

| Aspect | 64 Envs | 256 Envs | Verdict |
|--------|---------|----------|---------|
| **Memory** | 9 MB | 37 MB | ‚úÖ No problem |
| **GPU VRAM** | 2-4 GB | 8-16 GB | ‚ö†Ô∏è Need good GPU or use CPU |
| **Throughput** | 384 samples/sec | 512 samples/sec | ‚úÖ 33% faster |
| **PPO update time** | ~2-3 sec | ~8-12 sec | ‚ö†Ô∏è 4√ó slower |
| **Total training time** | ~8 min (100 epochs) | ~25 min (100 epochs) | ‚ö†Ô∏è 3√ó longer |
| **Data quality** | Good | Better (4√ó more data) | ‚úÖ Better gradients |
| **Hyperparameter tuning** | Not needed | Minor adjustments | ‚ö†Ô∏è Slight tuning helpful |

---

## Final Recommendation

‚úÖ **Yes, 256 envs should work fine!**

**What to do**:
1. Increase `num_episodes` proportionally (e.g., 25,600 for 100 epochs)
2. Use GPU with 16+ GB VRAM (or be prepared for slower CPU training)
3. Increase `batch_size` to 128 or 256
4. Reduce `num_value_epochs` and `num_policy_epochs` slightly
5. Monitor first few epochs to ensure no OOM or excessive slowness

**Expected benefits**:
- üöÄ Better gradient estimates (less variance)
- üìà Potentially better final performance
- üéØ More diverse experiences per update

**Trade-off**:
- ‚è±Ô∏è ~3√ó longer wall-clock time for same number of epochs
- üíæ Slightly higher memory usage (but still very manageable)

**Command to try**:
```bash
python meta_train.py --episodes 25600 --num-envs 256 --device cuda
```

üéâ **Go for it!**
